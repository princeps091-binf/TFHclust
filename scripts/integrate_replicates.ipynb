{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90121a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import pandas as pd\n",
    "import pyBigWig\n",
    "import hdbscan\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import bioframe as bf\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import t\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87272109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_reads(bam_file,chrom):\n",
    "    bamfile = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "    bam_iter = bamfile.fetch(chrom)\n",
    "\n",
    "    bam_dfs = []\n",
    "    for ix in bam_iter:\n",
    "        bam_dfs.append(pd.DataFrame({'ID':[ix.to_dict()['name']],'chrom':[ix.to_dict()['ref_name']],'start':[ix.to_dict()['ref_pos']],'flag':[ix.to_dict()['flag']]}))\n",
    "\n",
    "    bam_df = pd.concat(bam_dfs).assign(start = lambda df_: df_.start.astype(int))\n",
    "\n",
    "    return bam_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f11f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chrom_read_tbl_from_bam(bam_file,tmp_chrom,tmp_folder):\n",
    "    tmp_file = f\"{tmp_folder}/tmp.bam\"\n",
    "    tmp_index_file = f\"{tmp_folder}/tmp.bai\"\n",
    "    subprocess.run(['touch', tmp_file])\n",
    "    subprocess.run([\"samtools\", \"view\", \"-b\", \"-o\",tmp_file, bam_file, tmp_chrom])\n",
    "    subprocess.run([\"samtools\", \"index\", tmp_file, tmp_index_file])\n",
    "    tmp_bamfile = pysam.AlignmentFile(tmp_file, \"rb\")\n",
    "    obs_iter = tmp_bamfile.fetch(tmp_chrom)\n",
    "    obs_dfs = []\n",
    "    for ix in obs_iter:\n",
    "        obs_dfs.append(pd.DataFrame({'ID':[ix.to_dict()['name']],'chrom':[ix.to_dict()['ref_name']],'start':[ix.to_dict()['ref_pos']],'flag':[ix.to_dict()['flag']]}))\n",
    "    subprocess.run([\"rm\", tmp_file, tmp_index_file])\n",
    "    return pd.concat(obs_dfs)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f5d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_HDBScan_clustering(read_tbl,min_cluster,processes):\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=min_cluster,\n",
    "                        metric='euclidean',\n",
    "                        core_dist_n_jobs=processes)\n",
    "    clusterer.fit(read_tbl.loc[:,['start']])\n",
    "\n",
    "def collect_hdb_cluster_read(g):\n",
    "    \n",
    "    leaves = set([v for v, d in g.out_degree() if d == 0])\n",
    "    HDB_clusters = [v for v, d in g.out_degree() if d > 0]\n",
    "\n",
    "    cl_read_idx = [list(nx.descendants(g,i).intersection(leaves)) for i in HDB_clusters]\n",
    "    cl_read_tbl = pd.DataFrame({\"HDB_cluster\":HDB_clusters,\"read_id_set\":cl_read_idx})\n",
    "    return(cl_read_tbl)\n",
    "\n",
    "def build_hic_zscore(data_tbl,vars,vars_df,vars_degree,target_var):\n",
    "    \"\"\"\n",
    "        Computes statistical values (z-scores and p-values) for a given dataset using a smoothing model.\n",
    "\n",
    "        This function applies a Generalized Additive Model (GAM) with B-splines to analyze the relationship\n",
    "        between selected variables and a target variable. It returns the original dataset with two additional\n",
    "        columns: \n",
    "        - `zscore`: A measure of how much each data point deviates from the average.\n",
    "        - `pvalue`: A value indicating the significance of each data point.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_tbl : pandas.DataFrame\n",
    "            The dataset containing the variables to be analyzed.\n",
    "        \n",
    "        vars : list of str\n",
    "            The names of the columns to be used as predictors (independent variables).\n",
    "        \n",
    "        vars_df : int\n",
    "            The number of degrees of freedom for the spline smoothing.\n",
    "        \n",
    "        vars_degree : int\n",
    "            The degree of the spline function, which controls how smooth the curve is.\n",
    "        \n",
    "        target_var : str\n",
    "            The column name of the target variable (dependent variable) being analyzed.\n",
    "\n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            The original dataset with two additional columns:\n",
    "            - `zscore`: The standardized residuals (how different each value is from the model's prediction).\n",
    "            - `pvalue`: A probability score indicating the significance of each data point.\n",
    "        \n",
    "        Example:\n",
    "        --------\n",
    "        Suppose `data_tbl` contains genetic data, where `vars` represents different genetic markers, \n",
    "        and `target_var` is a measure of gene expression. This function helps determine which data \n",
    "        points significantly deviate from the expected pattern.\n",
    "    \"\"\"\n",
    "    x_spline = data_tbl[vars].to_numpy(dtype=float)\n",
    "    y = data_tbl[target_var].to_numpy(dtype=float)\n",
    "    bs = sm.gam.BSplines(x_spline, df=vars_df, degree=vars_degree)\n",
    "\n",
    "    chr_gam = sm.GLMGam(y,smoother=bs)\n",
    "    chr_gam_res = chr_gam.fit()\n",
    "    gam_infl = chr_gam_res.get_influence()\n",
    "    bs_tranform_exog = bs.transform(data_tbl[vars].to_numpy())\n",
    "    tmp_rng = chr_gam_res.get_distribution(exog=bs_tranform_exog)\n",
    "    mod_pvalue = tmp_rng.sf(data_tbl[target_var].to_numpy())\n",
    "    new_data_tbl = data_tbl.assign(zscore = gam_infl.resid_studentized,pvalue = mod_pvalue)\n",
    "    return new_data_tbl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3f42f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class read_clustering:\n",
    "    def __init__(self,read_df):\n",
    "        self.read_tbl = read_df\n",
    "        self.chrom = read_df.iloc[0,:].chrom\n",
    "        self.full_graph = None\n",
    "        self.cl_read_tbl = None\n",
    "        self.regression_tbl = None\n",
    "        self.res_tbl = None\n",
    "        self.summary_tbl = None\n",
    "    \n",
    "    def HDBScan_clustering(self,min_size,njobs):\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size=min_size,\n",
    "                        metric='euclidean',\n",
    "                        core_dist_n_jobs=njobs)\n",
    "        clusterer.fit(self.read_tbl.loc[:,['start']])\n",
    "        self.full_graph = clusterer.condensed_tree_.to_networkx()\n",
    "    \n",
    "    def collect_hdb_cluster_read(self):\n",
    "    \n",
    "        leaves = set([v for v, d in self.full_graph.out_degree() if d == 0])\n",
    "        HDB_clusters = [v for v, d in self.full_graph.out_degree() if d > 0]\n",
    "\n",
    "        cl_read_idx = [list(nx.descendants(self.full_graph,i).intersection(leaves)) for i in HDB_clusters]\n",
    "        tmp_cl_read_tbl = pd.DataFrame({\"HDB_cluster\":HDB_clusters,\"read_id_set\":cl_read_idx})\n",
    "        node_depth = nx.shortest_path_length(self.full_graph,source=tmp_cl_read_tbl.HDB_cluster.min())\n",
    "        tmp_cl_read_tbl = (tmp_cl_read_tbl\n",
    "                           .assign(lvl = lambda df_: [node_depth[i] for i in df_.HDB_cluster.to_list()])\n",
    "                           .assign(norm_lvl = lambda df_: df_.lvl/df_.lvl.max()))\n",
    "\n",
    "        long_cl_read_tbl = tmp_cl_read_tbl.explode('read_id_set')\n",
    "\n",
    "        long_cl_read_tbl = (long_cl_read_tbl\n",
    "                            .assign(start = lambda df_:self.read_tbl.reset_index().start.to_numpy()[df_.read_id_set.to_numpy(dtype=int)])\n",
    "                            )\n",
    "\n",
    "        self.cl_read_tbl = (long_cl_read_tbl\n",
    "                                        .groupby('HDB_cluster')\n",
    "                                        .agg(start = ('start','min'),\n",
    "                                            end=('start','max'),\n",
    "                                            rc = ('start','count')\n",
    "                                            )\n",
    "                                        .assign(w= lambda df_:df_.end - df_.start)\n",
    "                                    ).reset_index().merge(tmp_cl_read_tbl)\n",
    "\n",
    "    def  build_regression_tbl(self,ctrl_read_df):\n",
    "\n",
    "        bg_coord_df = ctrl_read_df.loc[:,['chrom','start']].assign(start = lambda df_: df_.start.astype(int)).assign(end = lambda df_: df_.start + 1)\n",
    "        hdb_cluster_bg_rc_df = bf.count_overlaps(self.cl_read_tbl.assign(chrom = self.chrom).loc[:,['chrom','start','end','HDB_cluster']],bg_coord_df)\n",
    "        tmp_chr_hdb_summary_tbl = self.cl_read_tbl.merge(hdb_cluster_bg_rc_df.rename(columns={'count':'bg_count'}))\n",
    "        \n",
    "        self.regression_tbl = (tmp_chr_hdb_summary_tbl\n",
    "                    .loc[:,['HDB_cluster','rc','bg_count']]\n",
    "                    .assign(lrc = lambda df_: np.log10(df_.rc), lbg = lambda df_: np.log10(df_.bg_count))\n",
    "                    .assign(lbg2 = lambda df_: np.where(df_.bg_count.lt(1),0,df_.lbg))\n",
    "                    )\n",
    "    def compute_zscore(self,explanatory_vars,vars_df,vard_degree,target_variable):\n",
    "        mod_res_tbl = build_hic_zscore(self.regression_tbl,explanatory_vars,vars_df,vard_degree,target_variable)\n",
    "        self.res_tbl = self.regression_tbl.merge(mod_res_tbl)\n",
    "        self.summary_tbl =   (self.res_tbl\n",
    "                                .merge(self.cl_read_tbl.assign(chrom = self.chrom).loc[:,['HDB_cluster','chrom','start','end','w','norm_lvl']])\n",
    "                                .assign(FC = lambda df_: (df_.rc/df_.w)/((df_.bg_count+1)/df_.w))\n",
    "                             )\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f660b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_cluster(rep1_cl_read_tbl,rep2_cl_read_tbl):\n",
    "\n",
    "    rep_overlap_df = (bf.overlap(rep1_cl_read_tbl.loc[:,['chrom','start','end','HDB_cluster']],\n",
    "                                 rep2_cl_read_tbl.loc[:,['chrom','start','end','HDB_cluster']],\n",
    "                                 return_overlap=True,\n",
    "                                 how='inner',suffixes=['_rep1','_rep2'])\n",
    "    .assign(inter_w=lambda df_:(df_.overlap_end-df_.overlap_start),\n",
    "            end_point=lambda df_:df_[[\"end_rep1\",\"end_rep2\"]].values.tolist(),\n",
    "            start_points = lambda df_:df_[[\"start_rep1\",\"start_rep2\"]].values.tolist())\n",
    "    .assign(jaccard = lambda df_:df_.inter_w/(df_.end_point.apply(max) - df_.start_points.apply(min)))\n",
    "    .drop(['end_point','start_points'],axis=1)\n",
    "    )\n",
    "\n",
    "    rep1_max_jaccard_idx = (rep_overlap_df\n",
    "            .groupby(['chrom_rep1','start_rep1','end_rep1','HDB_cluster_rep1'])\n",
    "            .jaccard.idxmax()\n",
    "            )\n",
    "\n",
    "    rep2_max_jaccard_idx = (rep_overlap_df\n",
    "            .groupby(['chrom_rep2','start_rep2','end_rep2','HDB_cluster_rep2'])\n",
    "            .jaccard.idxmax()\n",
    "            )\n",
    "\n",
    "    rep1_matching_tbl = rep_overlap_df.iloc[rep1_max_jaccard_idx,:]\n",
    "    rep2_matching_tbl = rep_overlap_df.iloc[rep2_max_jaccard_idx,:]\n",
    "\n",
    "    return (\n",
    "        rep1_matching_tbl,\n",
    "        rep2_matching_tbl\n",
    "        )\n",
    "\n",
    "def get_CvM_pvalue(i,tbl):\n",
    "    return stats.cramervonmises_2samp(tbl.iloc[i].rep1_start,tbl.iloc[i].rep2_start).pvalue\n",
    "\n",
    "def get_matched_cluster_agreement_pvalue(rep_match_tbl,rep1_clustering,rep2_clustering,njobs):\n",
    "    tmp_rep1_read_start_tbl = (rep_match_tbl.loc[:,['HDB_cluster_rep1']]\n",
    "    .drop_duplicates()\n",
    "    .merge(rep1_clustering.cl_read_tbl.loc[:,['HDB_cluster','read_id_set']],left_on = ['HDB_cluster_rep1'],right_on = ['HDB_cluster'])\n",
    "    .drop('HDB_cluster',axis=1)\n",
    "    .assign(DNA_start = lambda df_: df_.apply(lambda x:rep1_clustering.read_tbl.start.iloc[x.read_id_set].to_numpy(),axis=1))\n",
    "    .drop('read_id_set',axis=1)\n",
    "    .rename(columns={'DNA_start':\"rep1_start\"})\n",
    "    )\n",
    "\n",
    "    tmp_rep2_read_start_tbl = (rep_match_tbl.loc[:,['HDB_cluster_rep2']]\n",
    "    .drop_duplicates()\n",
    "    .merge(rep2_clustering.cl_read_tbl.loc[:,['HDB_cluster','read_id_set']],left_on = ['HDB_cluster_rep2'],right_on = ['HDB_cluster'])\n",
    "    .drop('HDB_cluster',axis=1)\n",
    "    .assign(DNA_start = lambda df_: df_.apply(lambda x:rep2_clustering.read_tbl.start.iloc[x.read_id_set].to_numpy(),axis=1))\n",
    "    .drop('read_id_set',axis=1)\n",
    "    .rename(columns={'DNA_start':\"rep2_start\"})\n",
    "    )\n",
    "\n",
    "    rep_match_read_coord_tbl = (rep_match_tbl\n",
    "    .loc[:,['HDB_cluster_rep1','HDB_cluster_rep2','jaccard']]\n",
    "    .merge(tmp_rep1_read_start_tbl)\n",
    "    .merge(tmp_rep2_read_start_tbl)\n",
    "    )\n",
    "\n",
    "    with multiprocessing.Pool(processes=njobs) as pool:\n",
    "            # Using map_async method to perform square operation on all numbers parallely\n",
    "            read_agreement_pvalue = pool.map(partial(get_CvM_pvalue, tbl = rep_match_read_coord_tbl),\n",
    "                                            range(rep_match_read_coord_tbl.shape[0])) \n",
    "    return rep_match_read_coord_tbl.assign(cvm_pvalue = read_agreement_pvalue).loc[:,['HDB_cluster_rep1','HDB_cluster_rep2','jaccard','cvm_pvalue']]   \n",
    "\n",
    "       \n",
    "class merged_clustering:\n",
    "    def __init__(self,rep1_clustering,rep2_clustering):\n",
    "        self.rep1 = rep1_clustering\n",
    "        self.rep2 = rep2_clustering\n",
    "        self.chrom = self.rep1.read_tbl.iloc[0,:].chrom\n",
    "        self.rep1_match_tbl = None\n",
    "        self.rep2_match_tbl = None\n",
    "        self.robust_cluster_coord_tbl = None\n",
    "        self.inter_rep_match_tbl = None\n",
    "        \n",
    "    def match_rep_cluster(self):\n",
    "        self.rep1_match_tbl, self.rep2_match_tbl = match_cluster(self.rep1.summary_tbl,self.rep2.summary_tbl)        \n",
    "\n",
    "    def evaluate_overlap_significance(self,njobs):\n",
    "            self.rep1_match_pvalue_tbl = get_matched_cluster_agreement_pvalue(self.rep1_match_tbl,self.rep1,self.rep2,njobs)\n",
    "            self.rep2_match_pvalue_tbl = get_matched_cluster_agreement_pvalue(self.rep2_match_tbl,self.rep1,self.rep2,njobs)\n",
    "\n",
    "\n",
    "    def filter_significant_overlap_cluster(self,CvM_thresh,specificity_thresh):\n",
    "        \n",
    "        rep1_matched_cluster_ID_list = self.rep1_match_pvalue_tbl.query(\"cvm_pvalue > @CvM_thresh\").HDB_cluster_rep1.drop_duplicates().to_list()\n",
    "        rep2_matched_cluster_ID_list = self.rep2_match_pvalue_tbl.query(\"cvm_pvalue > @CvM_thresh\").HDB_cluster_rep2.drop_duplicates().to_list()\n",
    "\n",
    "        rep1_robust_cluster_tbl = (self.rep1\n",
    "                                    .summary_tbl.loc[:,['HDB_cluster','chrom','start','end','zscore','pvalue']]\n",
    "                                    .query(\"HDB_cluster in @rep1_matched_cluster_ID_list\")\n",
    "                                    .assign(rep = \"rep1\")\n",
    "                                    .merge(self.rep1_match_pvalue_tbl,left_on='HDB_cluster',right_on=\"HDB_cluster_rep1\")\n",
    "                                    .drop('HDB_cluster',axis=1)\n",
    "                                    .query(\"HDB_cluster_rep2 in @rep2_matched_cluster_ID_list\")\n",
    "                                    .merge(self.rep2.summary_tbl.loc[:,['HDB_cluster','pvalue']].rename(columns={'HDB_cluster':'HDB_cluster_rep2','pvalue':'pvalue_rep2'}))\n",
    "                                    .query('pvalue < @specificity_thresh and pvalue_rep2 < @specificity_thresh')\n",
    "                                    .loc[:,['chrom','start','end','zscore','pvalue','rep','HDB_cluster_rep1','HDB_cluster_rep2','jaccard','cvm_pvalue']]\n",
    "                                )\n",
    "        rep2_robust_cluster_tbl = (self.rep2\n",
    "                                    .summary_tbl.loc[:,['HDB_cluster','chrom','start','end','zscore','pvalue']]\n",
    "                                    .query(\"HDB_cluster in @rep2_matched_cluster_ID_list\")\n",
    "                                    .assign(rep = \"rep2\")\n",
    "                                    .merge(self.rep2_match_pvalue_tbl,left_on='HDB_cluster',right_on=\"HDB_cluster_rep2\")\n",
    "                                    .drop('HDB_cluster',axis=1)\n",
    "                                    .query(\"HDB_cluster_rep1 in @rep1_matched_cluster_ID_list\")\n",
    "                                    .merge(self.rep1.summary_tbl.loc[:,['HDB_cluster','pvalue']].rename(columns={'HDB_cluster':'HDB_cluster_rep1','pvalue':'pvalue_rep1'}))\n",
    "                                    .query('pvalue < @specificity_thresh and pvalue_rep1 < @specificity_thresh')\n",
    "                                    .loc[:,['chrom','start','end','zscore','pvalue','rep','HDB_cluster_rep2','HDB_cluster_rep1','jaccard','cvm_pvalue']]\n",
    "                                )\n",
    "        if (rep1_robust_cluster_tbl.shape[0] > 0 and rep2_robust_cluster_tbl.shape[0]):\n",
    "            self.robust_cluster_match_tbl = bf.cluster(pd.concat([rep1_robust_cluster_tbl.drop('HDB_cluster_rep2',axis=1).rename(columns={'HDB_cluster_rep1':'HDB_cluster'}),\n",
    "                                                                  rep2_robust_cluster_tbl.drop('HDB_cluster_rep1',axis=1).rename(columns={'HDB_cluster_rep2':'HDB_cluster'})])).assign(w = lambda df_: df_.cluster_end -df_.start).sort_values('w')\n",
    "\n",
    "            self.robust_cluster_coord_tbl = (self.robust_cluster_match_tbl\n",
    "                            .loc[:,['chrom','cluster_start','cluster_end','cluster']]\n",
    "                            .drop_duplicates()\n",
    "                            .rename(columns = {'cluster_start':'start','cluster_end':\"end\"})\n",
    "                            .assign(w = lambda df_: df_.end - df_.start)\n",
    "                            )\n",
    "            self.inter_rep_match_tbl = (pd.concat([rep2_robust_cluster_tbl\n",
    "                                        .loc[:,['HDB_cluster_rep1','HDB_cluster_rep2','cvm_pvalue','jaccard']]\n",
    "                                        .merge(self.rep1.summary_tbl.loc[:,['HDB_cluster','chrom','start','end','zscore','pvalue','norm_lvl']]\n",
    "                                               .rename(columns = {'start':'start_rep1','end':'end_rep1','pvalue':'pvalue_rep1','zscore':'zscore_rep1','norm_lvl':'norm_lvl_rep1'}),left_on = 'HDB_cluster_rep1',right_on='HDB_cluster').drop('HDB_cluster',axis=1)\n",
    "                                        .merge(self.rep2.summary_tbl.loc[:,['HDB_cluster','start','end','zscore','pvalue','norm_lvl']]\n",
    "                                               .rename(columns = {'start':'start_rep2','end':'end_rep2','pvalue':'pvalue_rep2','zscore':'zscore_rep2','norm_lvl':'norm_lvl_rep2'}),left_on = 'HDB_cluster_rep2',right_on='HDB_cluster').drop('HDB_cluster',axis=1)\n",
    "                                        ,\n",
    "                                        rep1_robust_cluster_tbl\n",
    "                                        .loc[:,['HDB_cluster_rep1','HDB_cluster_rep2','cvm_pvalue','jaccard']]\n",
    "                                        .merge(self.rep1.summary_tbl.loc[:,['HDB_cluster','chrom','start','end','zscore','pvalue','norm_lvl']]\n",
    "                                               .rename(columns = {'start':'start_rep1','end':'end_rep1','pvalue':'pvalue_rep1','zscore':'zscore_rep1','norm_lvl':'norm_lvl_rep1'}),left_on = 'HDB_cluster_rep1',right_on='HDB_cluster').drop('HDB_cluster',axis=1)\n",
    "                                        .merge(self.rep2.summary_tbl.loc[:,['HDB_cluster','start','end','zscore','pvalue','norm_lvl']]\n",
    "                                               .rename(columns = {'start':'start_rep2','end':'end_rep2','pvalue':'pvalue_rep2','zscore':'zscore_rep2','norm_lvl':'norm_lvl_rep2'}),left_on = 'HDB_cluster_rep2',right_on='HDB_cluster').drop('HDB_cluster',axis=1)\n",
    "                                        ]).drop_duplicates()\n",
    "                                        .assign(start = lambda df_: df_.apply(lambda row: min(row.start_rep1,row.start_rep2),axis=1),\n",
    "                                                end = lambda df_: df_.apply(lambda row: max(row.end_rep1,row.end_rep2),axis=1))\n",
    "                                        .assign(w = lambda df_: df_.end -df_.start))\n",
    "        else:\n",
    "            print(\"no significant and replicable cluster\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afd3f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_bam = \"./../data/H3K27ac/bam/H3K27AC_ctrl_MCF7_ENCFF506VER.bam\"\n",
    "rep1_bam = \"./../data/H3K27ac/bam/H3K27ac_obs_MCF7_ENCFF355TOT.bam\"\n",
    "rep2_bam = \"./../data/H3K27ac/bam/H3K27ac_obs_MCF7_ENCFF422VIN.bam\"\n",
    "\n",
    "pvalue_bw_file = \"./../data/CTCF/bigwig/CTCF_pvalue_MCF7_ENCFF767VZW.bigWig\"\n",
    "rep1_peak_bed_file = \"./../data/H3K27me3/bed/H3K27me3_peak_MCF7_rep1_ENCFF035BIQ.bed\"\n",
    "rep2_peak_bed_file = \"./../data/H3K27ac/bed/H3K27AC_IDR_peak_ENCFF340KSH.bed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0f287b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_chrom = 'chr19'\n",
    "bg_df = import_reads(ctrl_bam,tmp_chrom)\n",
    "rep1_df = import_reads(rep1_bam,tmp_chrom)\n",
    "rep2_df = import_reads(rep2_bam,tmp_chrom)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24ff82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_chrom = 'chr19'\n",
    "bg_df = get_chrom_read_tbl_from_bam(ctrl_bam,tmp_chrom,\"./../data/tmp\").assign(start = lambda df_: df_.start.astype(int))\n",
    "rep1_df = get_chrom_read_tbl_from_bam(rep1_bam,tmp_chrom,\"./../data/tmp\").assign(start = lambda df_: df_.start.astype(int))\n",
    "rep2_df = get_chrom_read_tbl_from_bam(rep2_bam,tmp_chrom,\"./../data/tmp\").assign(start = lambda df_: df_.start.astype(int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d87aed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "peak_df = (pd.read_csv(rep2_peak_bed_file,sep=\"\\t\",header=None,usecols=[0,1,2,3,4])\n",
    " .rename(columns={0:'chrom',1:'start',2:'end',3:'ID',4:'IDR'})\n",
    " )\n",
    "\n",
    "rep2_peak_df = (pd.read_csv(rep2_peak_bed_file,sep=\"\\t\",header=None,usecols=[0,1,2,3,4])\n",
    " .rename(columns={0:'chrom',1:'start',2:'end',3:'ID',4:'IDR'})\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707bd3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep1_clustering = read_clustering(rep1_df)\n",
    "rep1_clustering.HDBScan_clustering(3,10)\n",
    "rep1_clustering.collect_hdb_cluster_read()\n",
    "rep1_clustering.build_regression_tbl(bg_df)\n",
    "rep1_clustering.compute_zscore(['lbg2'],[10],[3],'lrc')\n",
    "\n",
    "rep2_clustering = read_clustering(rep2_df)\n",
    "rep2_clustering.HDBScan_clustering(3,10)\n",
    "rep2_clustering.collect_hdb_cluster_read()\n",
    "rep2_clustering.build_regression_tbl(bg_df)\n",
    "rep2_clustering.compute_zscore(['lbg2'],[10],[3],'lrc')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5945beb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_hic_clustering = merged_clustering(rep1_clustering,rep2_clustering)\n",
    "merged_hic_clustering.match_rep_cluster()\n",
    "merged_hic_clustering.evaluate_overlap_significance(10)\n",
    "merged_hic_clustering.filter_significant_overlap_cluster(0.5,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc34254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_hic_clustering.inter_rep_match_tbl.sort_values('pvalue_rep1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c32c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_hic_clustering.inter_rep_match_tbl.jaccard.plot.kde()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e804f764",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_hic_clustering.inter_rep_match_tbl.plot.scatter(x='norm_lvl_rep1',y='norm_lvl_rep2',s=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bf76f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth_window = 150\n",
    "rep1_obs_rcount_bdg = pd.concat([pd.DataFrame({'DNA_pos':rep1_df.start.to_numpy() }).assign(score = 1),\n",
    "                             pd.DataFrame({'DNA_pos':rep1_df.start.to_numpy() + smooth_window}).assign(score = -1)\n",
    "                            ]).sort_values('DNA_pos').assign(csum = lambda df_: df_.score.cumsum()).groupby('DNA_pos').agg(rcount = ('csum','max')).reset_index()\n",
    "rep2_obs_rcount_bdg = pd.concat([pd.DataFrame({'DNA_pos':rep2_df.start.to_numpy() }).assign(score = 1),\n",
    "                             pd.DataFrame({'DNA_pos':rep2_df.start.to_numpy() + smooth_window}).assign(score = -1)\n",
    "                            ]).sort_values('DNA_pos').assign(csum = lambda df_: df_.score.cumsum()).groupby('DNA_pos').agg(rcount = ('csum','max')).reset_index()\n",
    "\n",
    "bg_rcount_bdg = pd.concat([pd.DataFrame({'DNA_pos':bg_df.start.to_numpy() }).assign(score = 1),\n",
    "                             pd.DataFrame({'DNA_pos':bg_df.start.to_numpy() + smooth_window}).assign(score = -1)\n",
    "                            ]).sort_values('DNA_pos').assign(csum = lambda df_: df_.score.cumsum()).groupby('DNA_pos').agg(rcount = ('csum','max')).reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3689fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rep1_obs_rcount_bdg = rep1_obs_rcount_bdg.assign(rd = lambda df_: df_.rcount/rep1_df.shape[0])\n",
    "rep2_obs_rcount_bdg = rep2_obs_rcount_bdg.assign(rd = lambda df_: df_.rcount/rep2_df.shape[0])\n",
    "bg_rcount_bdg =bg_rcount_bdg.assign(rd = lambda df_: df_.rcount/bg_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd66a5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_clusters_df = merged_hic_clustering.inter_rep_match_tbl.assign(lvl = lambda df_: (df_.norm_lvl_rep1 + df_.norm_lvl_rep2)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af4b685",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,5))\n",
    "gs = fig.add_gridspec(3, 1, height_ratios=(1, 1, 1),\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.05, hspace=0.05)\n",
    "# plot_window_min = rep1_obs_rcount_bdg.DNA_pos.min()\n",
    "# plot_window_max = rep1_obs_rcount_bdg.DNA_pos.max()\n",
    "\n",
    "plot_window_min = 41_500_000\t\t\t\t \n",
    "plot_window_max = 43_000_000\n",
    "\n",
    "ax_dens = fig.add_subplot(gs[0, 0],xlim = (plot_window_min,plot_window_max) )\n",
    "ax_dens.xaxis.set_visible(False)\n",
    "# ax_dens.set_yscale('log')\n",
    "ax_dens.set_ylabel('read density')\n",
    "\n",
    "ax_agg = fig.add_subplot(gs[1, 0],sharex=ax_dens)\n",
    "ax_agg.xaxis.set_major_formatter(lambda x,pos: (str(np.round(x/1e6,decimals=2)) + \" Mb\"))\n",
    "ax_agg.xaxis.set_visible(False)\n",
    "ax_agg.set_ylabel('MACS')\n",
    "\n",
    "ax_cl = fig.add_subplot(gs[2, 0],sharex=ax_dens)\n",
    "ax_cl.xaxis.set_major_formatter(lambda x,pos: (str(np.round(x/1e6,decimals=2)) + \" Mb\"))\n",
    "ax_cl.xaxis.set_visible(True)\n",
    "ax_cl.set_ylabel('depth')\n",
    "\n",
    "for bin_idx in range(candidate_clusters_df.shape[0]):\n",
    "    tmp_cl =  candidate_clusters_df.iloc[bin_idx,:]\n",
    "    tmp_start = tmp_cl.start\n",
    "    tmp_end = tmp_cl.end \n",
    "    tmp_h = tmp_cl.lvl\n",
    "    ax_cl.hlines(y=tmp_h,xmin=tmp_start,xmax=tmp_end,color='black',linewidth=1)\n",
    "\n",
    "tmp_peak_df = bf.merge(peak_df.query('chrom == \"chr19\"'))\n",
    "for bin_idx in range(tmp_peak_df.shape[0]):\n",
    "    tmp_cl =  tmp_peak_df.iloc[bin_idx,:]\n",
    "    tmp_start = tmp_cl.start\n",
    "    tmp_end = tmp_cl.end \n",
    "    # tmp_h = tmp_cl.n_intervals\n",
    "    if bin_idx %2 == 0:\n",
    "        ax_agg.hlines(y=0.5,xmin=tmp_start,xmax=tmp_end,color='black',linewidth=30)\n",
    "    else:\n",
    "        ax_agg.hlines(y=0.5,xmin=tmp_start,xmax=tmp_end,color='silver',linewidth=30)\n",
    "ax_dens.step(x=rep1_obs_rcount_bdg.DNA_pos.to_numpy(),y=rep1_obs_rcount_bdg.rd.to_numpy(),where='post',linewidth=0.1,alpha=1,c='crimson')\n",
    "ax_dens.step(x=bg_rcount_bdg.DNA_pos.to_numpy(),y=bg_rcount_bdg.rd.to_numpy(),where='post',linewidth=0.1,alpha=1,c='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fb6f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "(bf.cluster(candidate_clusters_df.loc[:,['chrom','start','end','pvalue_rep1','pvalue_rep2','zscore_rep1','zscore_rep2','lvl','w']])\n",
    ".assign(cluster_w = lambda df_: df_.cluster_end -df_.start)\n",
    ".sort_values('cluster_w')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a73e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bf.count_overlaps(peak_df.query('chrom == \"chr22\"'),candidate_clusters_df).assign(in_agg = lambda df_: df_.loc[:,'count'].gt(0)).query(\"IDR >= 100\").in_agg.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906e51f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_hic_clustering.inter_rep_match_tbl.plot.scatter(x='zscore_rep1',y='zscore_rep2',s=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624534ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_hic_clustering.inter_rep_match_tbl.assign(avg_lvl = lambda df_: (df_.norm_lvl_rep1 + df_.norm_lvl_rep2)/2).plot.scatter(x='avg_lvl',y='w',logy=True,s=1,alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def produce_cluster_size_quantile_curve_df(res_tbl,lvl_vec_length,window_width):\n",
    "    expectile_tbl = res_tbl.assign(avg_lvl = lambda df_: (df_.norm_lvl_rep1 + df_.norm_lvl_rep2)/2)\n",
    "    lvl_vec = np.linspace(expectile_tbl.avg_lvl.min(),expectile_tbl.avg_lvl.max(),lvl_vec_length)\n",
    "    unique_lvl_vec = expectile_tbl.avg_lvl.drop_duplicates().sort_values().to_numpy()\n",
    "    window_size = int(np.ceil(len(unique_lvl_vec) * (window_width)))\n",
    "    nbrs = NearestNeighbors(n_neighbors=window_size, algorithm='ball_tree').fit(unique_lvl_vec.reshape(-1, 1))\n",
    "    lvl_nn_array = nbrs.kneighbors(lvl_vec.reshape(-1, 1))[1]\n",
    "    dfs = []\n",
    "    for tmp_lvl_idx in range(lvl_nn_array.shape[0]):\n",
    "        tmp_lvl_window = unique_lvl_vec[lvl_nn_array[tmp_lvl_idx,:],]\n",
    "        dfs.append(expectile_tbl.query(\"avg_lvl in @tmp_lvl_window\").w.quantile(np.linspace(0.1,0.9,5)).reset_index().rename(columns={'index':'quantile'}).assign(lvl = lvl_vec[tmp_lvl_idx]))\n",
    "    \n",
    "    return pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46b8aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linspace(0.1,0.9,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420c7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantile_curves_df = produce_cluster_size_quantile_curve_df(merged_hic_clustering.inter_rep_match_tbl,100,0.1)\n",
    "cmap = plt.get_cmap('cividis')\n",
    "norm = plt.Normalize(quantile_curves_df.loc[:,'quantile'].min(), quantile_curves_df.loc[:,'quantile'].max())\n",
    "line_colors = plt.cm.cividis(norm(quantile_curves_df.loc[:,'quantile'].drop_duplicates().to_numpy()))\n",
    "line_color_dict = {quantile_curves_df.loc[:,'quantile'].drop_duplicates().sort_values().to_numpy()[i]:line_colors[int(i),:] for i in range(line_colors.shape[0])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595ae380",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "gs = fig.add_gridspec(1, 1,\n",
    "                      left=0.1, right=0.9, bottom=0.1, top=0.9,\n",
    "                      wspace=0.05, hspace=0.05)\n",
    "plot_window_min = quantile_curves_df.lvl.min()\n",
    "plot_window_max = quantile_curves_df.lvl.max()\n",
    "\n",
    "ax_curve = fig.add_subplot(gs[0, 0],xlim = (plot_window_min,plot_window_max) )\n",
    "ax_curve.set_yscale('log')\n",
    "ax_curve.set_ylabel('cluster size (bp)')\n",
    "ax_curve.set_xlabel('cluster depth')\n",
    "\n",
    "for quant_val in quantile_curves_df.loc[:,'quantile'].drop_duplicates().to_list():\n",
    "    tmp_curve =  quantile_curves_df.query(\"quantile == @quant_val\").sort_values('lvl')\n",
    "    ax_curve.plot(tmp_curve.lvl.to_numpy(),tmp_curve.w.to_numpy(),color='black',linewidth=1)\n",
    "ax_curve.hlines(y=1_000,xmin=plot_window_min,xmax=plot_window_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbed5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    bf.merge(\n",
    "        bf.overlap(bf.merge(merged_hic_clustering.inter_rep_match_tbl.loc[:,['chrom','start','end','w']]).assign(w = lambda df_: df_.end-df_.start),rep1_peak_df,how='inner',return_overlap=True)\n",
    "        .loc[:,['chrom','overlap_start','overlap_end']]\n",
    "        .rename(columns = {'overlap_start':'start','overlap_end':'end'})\n",
    "            )\n",
    "    .assign(w = lambda df_: df_.end -df_.start)\n",
    "    .w.sum()/bf.merge(rep1_peak_df).assign(w = lambda df_: df_.end -df_.start).w.sum()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "51ee977d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4326.737704918033)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf.merge(merged_hic_clustering.inter_rep_match_tbl.loc[:,['chrom','start','end','w']]).assign(w = lambda df_: df_.end-df_.start).w.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5f2f8d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(228.35037114050147)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bf.merge(rep1_peak_df).assign(w = lambda df_: df_.end -df_.start).w.mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFHclust",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
